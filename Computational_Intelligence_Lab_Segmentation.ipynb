{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "64YSZEYHvLWz",
    "tags": []
   },
   "source": [
    "# Road Segmentation Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFdmXX32vN9G",
    "outputId": "4fb24ce1-b69e-4dc7-e3fd-6d66a61a4d02"
   },
   "outputs": [],
   "source": [
    "# Can skip in Jupyter\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9k5B9S2vui5"
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "PATCH_SIZE = 16  # pixels per side of square patches\n",
    "VAL_SIZE = 10  # size of the validation set (number of images)\n",
    "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mywPp16Gv3fI"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from random import sample\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "R38lPWswwrtZ",
    "tags": []
   },
   "source": [
    "## Helper functions\n",
    "These are some general utility functions for visualization and submission --> Directly taken from the provided project notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbmdvLa_v6Gq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_all_from_path(path):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
    "\n",
    "\n",
    "def show_first_n(imgs, masks, n=5):\n",
    "    # visualizes the first n elements of a series of images and segmentation masks\n",
    "    imgs_to_draw = min(5, len(imgs))\n",
    "    fig, axs = plt.subplots(2, imgs_to_draw, figsize=(18.5, 6))\n",
    "    for i in range(imgs_to_draw):\n",
    "        axs[0, i].imshow(imgs[i])\n",
    "        axs[1, i].imshow(masks[i])\n",
    "        axs[0, i].set_title(f'Image {i}')\n",
    "        axs[1, i].set_title(f'Mask {i}')\n",
    "        axs[0, i].set_axis_off()\n",
    "        axs[1, i].set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "def image_to_patches(images, masks=None):\n",
    "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
    "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
    "    n_images = images.shape[0]  # number of images\n",
    "    h, w = images.shape[1:3]  # shape of images\n",
    "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
    "\n",
    "    images = images[:,:,:,:3]\n",
    "\n",
    "    h_patches = h // PATCH_SIZE\n",
    "    w_patches = w // PATCH_SIZE\n",
    "\n",
    "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
    "    patches = np.moveaxis(patches, 2, 3)\n",
    "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
    "    if masks is None:\n",
    "        return patches\n",
    "\n",
    "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
    "    masks = np.moveaxis(masks, 2, 3)\n",
    "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
    "    labels = labels.reshape(-1).astype(np.float32)\n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "def show_patched_image(patches, labels, h_patches=25, w_patches=25):\n",
    "    # reorders a set of patches in their original 2D shape and visualizes them\n",
    "    fig, axs = plt.subplots(h_patches, w_patches, figsize=(18.5, 18.5))\n",
    "    for i, (p, l) in enumerate(zip(patches, labels)):\n",
    "        # the np.maximum operation paints patches labeled as road red\n",
    "        axs[i // w_patches, i % w_patches].imshow(np.maximum(p, np.array([l.item(), 0., 0.])))\n",
    "        axs[i // w_patches, i % w_patches].set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "def create_submission(test_pred, test_filenames, submission_filename):\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn, patch_array in zip(sorted(test_filenames), test_pred):\n",
    "            img_number = int(re.search(r\"satimage_(\\d+)\", fn).group(1))\n",
    "            for i in range(patch_array.shape[0]):\n",
    "                for j in range(patch_array.shape[1]):\n",
    "                    f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, j*PATCH_SIZE, i*PATCH_SIZE, int(patch_array[i, j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnfnZzVxwJLX"
   },
   "outputs": [],
   "source": [
    "#ROOT_PATH = \"/content/drive/MyDrive/Colab Notebooks/ethz-cil-road-segmentation-2024\"\n",
    "ROOT_PATH = \"ethz-cil-road-segmentation-2024\"\n",
    "images = load_all_from_path(os.path.join(ROOT_PATH, 'training', 'images'))[:, :, :, :3]\n",
    "masks = load_all_from_path(os.path.join(ROOT_PATH, 'training', 'groundtruth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wi08q44xhKE"
   },
   "source": [
    "## Helper Functions for image processing\n",
    "These are the utility functions used for image processing --> Directly taken from the provided project notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amPtu38YwZdo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def np_to_tensor(x, device):\n",
    "    # allocates tensors from np.arrays\n",
    "    if device == 'cpu':\n",
    "        return torch.from_numpy(x).cpu()\n",
    "    else:\n",
    "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    # dataset class that deals with loading the data and making it available by index.\n",
    "\n",
    "    def __init__(self, is_train, device, use_patches=True, resize_to=(400, 400)):\n",
    "        self.is_train = is_train\n",
    "        self.device = device\n",
    "        self.use_patches = use_patches\n",
    "        self.resize_to=resize_to\n",
    "        self.x, self.y, self.n_samples = None, None, None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):  # not very scalable, but good enough for now\n",
    "        self.x = train_images if self.is_train else val_images\n",
    "        self.y = train_masks if self.is_train else val_masks\n",
    "        if self.use_patches:  # split each image into patches\n",
    "            self.x, self.y = image_to_patches(self.x, self.y)\n",
    "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
    "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
    "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
    "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
    "        self.n_samples = len(self.x)\n",
    "\n",
    "    def _preprocess(self, x, y):\n",
    "        # to keep things simple we will not apply transformations to each sample,\n",
    "        # but it would be a very good idea to look into preprocessing\n",
    "        return x, y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "def show_val_samples(x, y, y_hat, segmentation=False):\n",
    "    # training callback to show predictions on validation set\n",
    "    imgs_to_draw = min(5, len(x))\n",
    "    if x.shape[-2:] == y.shape[-2:]:  # segmentation\n",
    "        fig, axs = plt.subplots(3, imgs_to_draw, figsize=(18.5, 12))\n",
    "        for i in range(imgs_to_draw):\n",
    "            axs[0, i].imshow(np.moveaxis(x[i], 0, -1))\n",
    "            axs[1, i].imshow(np.concatenate([np.moveaxis(y_hat[i], 0, -1)] * 3, -1))\n",
    "            axs[2, i].imshow(np.concatenate([np.moveaxis(y[i], 0, -1)]*3, -1))\n",
    "            axs[0, i].set_title(f'Sample {i}')\n",
    "            axs[1, i].set_title(f'Predicted {i}')\n",
    "            axs[2, i].set_title(f'True {i}')\n",
    "            axs[0, i].set_axis_off()\n",
    "            axs[1, i].set_axis_off()\n",
    "            axs[2, i].set_axis_off()\n",
    "    else:  # classification\n",
    "        fig, axs = plt.subplots(1, imgs_to_draw, figsize=(18.5, 6))\n",
    "        for i in range(imgs_to_draw):\n",
    "            axs[i].imshow(np.moveaxis(x[i], 0, -1))\n",
    "            axs[i].set_title(f'True: {np.round(y[i]).item()}; Predicted: {np.round(y_hat[i]).item()}')\n",
    "            axs[i].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEutq2eSxst4"
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs):\n",
    "    # training loop\n",
    "    logdir = './tensorboard/net'\n",
    "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
    "\n",
    "    history = {}  # collects metrics at the end of each epoch\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # initialize metric list\n",
    "        metrics = {'loss': [], 'val_loss': []}\n",
    "        for k, _ in metric_fns.items():\n",
    "            metrics[k] = []\n",
    "            metrics['val_'+k] = []\n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
    "        # training\n",
    "        model.train()\n",
    "        for (x, y) in pbar:\n",
    "            optimizer.zero_grad()  # zero out gradients\n",
    "            y_hat = model(x)  # forward pass\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()  # backward pass\n",
    "            optimizer.step()  # optimize weights\n",
    "\n",
    "            # log partial metrics\n",
    "            metrics['loss'].append(loss.item())\n",
    "            for k, fn in metric_fns.items():\n",
    "                metrics[k].append(fn(y_hat, y).item())\n",
    "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # do not keep track of gradients\n",
    "            for (x, y) in eval_dataloader:\n",
    "                y_hat = model(x)  # forward pass\n",
    "                loss = loss_fn(y_hat, y)\n",
    "\n",
    "                # log partial metrics\n",
    "                metrics['val_loss'].append(loss.item())\n",
    "                for k, fn in metric_fns.items():\n",
    "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
    "\n",
    "        # summarize metrics, log to tensorboard and display\n",
    "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
    "        for k, v in history[epoch].items():\n",
    "          writer.add_scalar(k, v, epoch)\n",
    "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
    "        show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
    "\n",
    "    print('Finished Training')\n",
    "    # plot loss curves\n",
    "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
    "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P_xLbD1x-sD"
   },
   "source": [
    "# Baseline 1 - U-Net\n",
    "This is the provided baseline U-Net with F1 score of 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7SJyj2Sx1qR"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
    "    def __init__(self, in_ch, out_ch, activation='RELU'):\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU() if activation == 'RELU' else nn.ELU()\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.BatchNorm2d(out_ch),\n",
    "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "                                   self.activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    # UNet-like architecture for single class semantic segmentation.\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024), activation='RELU'):\n",
    "        super().__init__()\n",
    "        enc_chs = chs  # number of channels in the encoder\n",
    "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
    "        if activation == 'RELU':\n",
    "            self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
    "            self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks   \n",
    "        else:\n",
    "            self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch, 'ELU') for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
    "            self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch, 'ELU') for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks   \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
    "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        enc_features = []\n",
    "        for block in self.enc_blocks[:-1]:\n",
    "            x = block(x) # pass through the block\n",
    "            enc_features.append(x)  # save features for skip connections\n",
    "            x = self.pool(x)  # decrease resolution\n",
    "        x = self.enc_blocks[-1](x)\n",
    "        # decode\n",
    "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
    "            x = upconv(x)  # increase resolution\n",
    "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
    "            x = block(x)  # pass through the block\n",
    "        return self.head(x)  # reduce to 1 channel\n",
    "\n",
    "\n",
    "def patch_accuracy_fn(y_hat, y):\n",
    "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
    "    h_patches = y.shape[-2] // PATCH_SIZE\n",
    "    w_patches = y.shape[-1] // PATCH_SIZE\n",
    "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    return (patches == patches_hat).float().mean()\n",
    "\n",
    "def accuracy_fn(y_hat, y):\n",
    "    # computes classification accuracy\n",
    "    return (y_hat.round() == y.round()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 2 - ResU-Net --> Road Extraction by Deep Residual U-Net\n",
    "This is the provided baseline U-Net with F1 score of 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, stride, padding):\n",
    "        super(ResidualConv, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.conv_skip(x)\n",
    "        \n",
    "class ResUnet(nn.Module):\n",
    "    def __init__(self, channel, filters=[64, 128, 256, 512]):\n",
    "        super(ResUnet, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(filters[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.input_skip = nn.Sequential(\n",
    "            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.residual_conv_1 = ResidualConv(filters[0], filters[1], 2, 1)\n",
    "        self.residual_conv_2 = ResidualConv(filters[1], filters[2], 2, 1)\n",
    "\n",
    "        self.bridge = ResidualConv(filters[2], filters[3], 2, 1)\n",
    "\n",
    "        self.upsample_1 = nn.ConvTranspose2d(filters[3], filters[3], 2, 2)\n",
    "        self.up_residual_conv1 = ResidualConv(filters[3] + filters[2], filters[2], 1, 1)\n",
    "\n",
    "        self.upsample_2 = nn.ConvTranspose2d(filters[2], filters[2], 2, 2)\n",
    "        self.up_residual_conv2 = ResidualConv(filters[2] + filters[1], filters[1], 1, 1)\n",
    "\n",
    "        self.upsample_3 = nn.ConvTranspose2d(filters[1], filters[1], 2, 2)\n",
    "        self.up_residual_conv3 = ResidualConv(filters[1] + filters[0], filters[0], 1, 1)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(filters[0], 1, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x1 = self.input_layer(x) + self.input_skip(x)\n",
    "        x2 = self.residual_conv_1(x1)\n",
    "        x3 = self.residual_conv_2(x2)\n",
    "        # Bridge\n",
    "        x4 = self.bridge(x3)\n",
    "        # Decode\n",
    "        x4 = self.upsample_1(x4)\n",
    "        x5 = torch.cat([x4, x3], dim=1)\n",
    "\n",
    "        x6 = self.up_residual_conv1(x5)\n",
    "\n",
    "        x6 = self.upsample_2(x6)\n",
    "        x7 = torch.cat([x6, x2], dim=1)\n",
    "\n",
    "        x8 = self.up_residual_conv2(x7)\n",
    "\n",
    "        x8 = self.upsample_3(x8)\n",
    "        x9 = torch.cat([x8, x1], dim=1)\n",
    "\n",
    "        x10 = self.up_residual_conv3(x9)\n",
    "\n",
    "        output = self.output_layer(x10)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MdOoynIyRzB"
   },
   "source": [
    "# Upgrade 1 - Using Transfer Learning for the Encoder\n",
    "In the architecture of the U-Net, the encoder is replaced with pretrained VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIIHl3T8yfdA"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "class UNetVGG(nn.Module):\n",
    "    def __init__(self, vgg_features, chs=(64,64,128,256,512,512)):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(*vgg_features[0:5])   # Conv1 (2 conv layers + maxpool)\n",
    "        self.enc2 = nn.Sequential(*vgg_features[5:10])  # Conv2 (2 conv layers + maxpool)\n",
    "        self.enc3 = nn.Sequential(*vgg_features[10:17]) # Conv3 (3 conv layers + maxpool)\n",
    "        self.enc4 = nn.Sequential(*vgg_features[17:24]) # Conv4 (3 conv layers + maxpool)\n",
    "        self.enc5 = nn.Sequential(*vgg_features[24:31]) # Conv5 (3 conv layers + maxpool)\n",
    "        self.encoders = [self.enc1, self.enc2, self.enc3, self.enc4, self.enc5]\n",
    "\n",
    "        dec_chs = chs[::-1]  # decoder channels in the reverse order\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
    "        self.dec_blocks = nn.ModuleList([Block(2*out_ch, out_ch) for out_ch in dec_chs[1:-1]])\n",
    "        self.dec_blocks.append(Block(64,64))\n",
    "        self.head = nn.Sequential(nn.Conv2d(64, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        enc_features = []\n",
    "        output = x\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "          output = encoder(output)\n",
    "          enc_features.append(output)\n",
    "\n",
    "        # decode\n",
    "        output = self.upconvs[0](enc_features[-1])\n",
    "        output = torch.cat((output, enc_features[3]), dim=1)\n",
    "        output = self.dec_blocks[0](output)\n",
    "\n",
    "        for block, upconv, feature in zip(self.dec_blocks[1:-1], self.upconvs[1:-1], enc_features[::-1][2:]):\n",
    "          output = upconv(output)\n",
    "          output = torch.cat((output, feature), dim=1)\n",
    "          output = block(output)\n",
    "\n",
    "        output = self.upconvs[-1](output)\n",
    "        output = self.dec_blocks[-1](output)\n",
    "        return self.head(output)  # reduce to 1 channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWSMggrO5gkJ"
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwCYYvcr5jop"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 4\n",
    "RESIZE = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG60BN6e5R61"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=0.2, random_state=42\n",
    ")\n",
    "train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
    "val_patches, val_labels = image_to_patches(val_images, val_masks)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d355a1a2a252455f90d5a2ef861f2d15",
      "1aae38846f6a4e98bb6eb9aa0db9ed1e",
      "47739ef169734cbab8629ea09b647e3d",
      "07545bbbe26846a8a71584d78aeabe8d",
      "bf6c4c21f6b84304bfca7d513133d6d9",
      "b7cbda34b30e4f3ebb2b9ec87b2cbf70",
      "a153cff3490644ffbc8411a98b918278",
      "2b48843e344546b2823b412e2dcf7a32",
      "b09cf406026b4503b6664668151f48d9",
      "d0d20d0c0edf4ddd82d020c610b0e521",
      "c017935d04394cb6b204fe70237475f7"
     ]
    },
    "id": "p9WwU3_q7Avz",
    "outputId": "478143b0-707a-4d47-bdbb-0039d96b1f24"
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "# Freeze all the layers\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze some of the later layers (uncomment if needed)\n",
    "for param in vgg16.features[24:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "vgg_features = list(vgg16.features.children())\n",
    "unet = UNetVGG(vgg_features)\n",
    "model = unet.to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "n_epochs = N_EPOCHS\n",
    "train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_CiVV_Y9hsa"
   },
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "test_path = os.path.join(ROOT_PATH, 'test', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(RESIZE, RESIZE)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
    "test_pred = np.moveaxis(test_pred, 2, 3)\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
    "create_submission(test_pred, test_filenames, submission_filename='unet_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-bVwCzV_WOH"
   },
   "source": [
    "## Upgrade 1.2 - Using Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4aRob6u564u"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_ESTIMATORS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDXRlMuCNZB7"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchvision import models\n",
    "bagging_models = []\n",
    "\n",
    "for i in range(N_ESTIMATORS):\n",
    "    vgg16 = models.vgg16(pretrained=True)\n",
    "    # Freeze all the layers\n",
    "    for param in vgg16.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Optionally, unfreeze some of the later layers (uncomment if needed)\n",
    "    for param in vgg16.features[24:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    vgg_features = list(vgg16.features.children())\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images, masks, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
    "    val_patches, val_labels = image_to_patches(val_images, val_masks)\n",
    "\n",
    "    # reshape the image to simplify the handling of skip connections and maxpooling\n",
    "    train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "    val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    bagging_models.append(UNetVGG(vgg_features).to(device))\n",
    "    loss_fn = nn.BCELoss()\n",
    "    metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
    "    optimizer = torch.optim.Adam(bagging_models[i].parameters())\n",
    "    train(train_dataloader, val_dataloader, bagging_models[i], loss_fn, metric_fns, optimizer, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTPxnVr0NZB7"
   },
   "outputs": [],
   "source": [
    "# Creating the predictions\n",
    "from collections import Counter\n",
    "predictions = []\n",
    "for model in bagging_models:\n",
    "    test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "    test_pred = np.concatenate(test_pred, 0)\n",
    "    test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "    test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "    # now compute labels\n",
    "    test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
    "    test_pred = np.moveaxis(test_pred, 2, 3)\n",
    "    test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
    "    predictions.append(test_pred)\n",
    "\n",
    "stacks = np.stack((predictions[0], predictions[1], predictions[2]), dim=0).astype(int)\n",
    "majority = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=stacks)\n",
    "create_submission(majority, test_filenames, submission_filename='unet_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbMMIiFUNZB8"
   },
   "source": [
    "## Upgrade 2 - CGAN --> https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8628717\n",
    "In the paper, they use a simple Unet architecture. I tried transfer learning in this part (did not give a better score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4agAIC_NZB8"
   },
   "outputs": [],
   "source": [
    "# Discriminator network for the GANS\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_YCKv4nNZB8"
   },
   "outputs": [],
   "source": [
    "# Losses for the discriminator and the generator\n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self, alpha=100):\n",
    "        super().__init__()\n",
    "        self.alpha=alpha\n",
    "        self.bce=nn.BCEWithLogitsLoss()\n",
    "        self.l1=nn.L1Loss()\n",
    "\n",
    "    def forward(self, fake, real, fake_pred):\n",
    "        fake_target = torch.ones_like(fake_pred)\n",
    "        loss = self.bce(fake_pred, fake_target) + self.alpha* self.l1(fake, real)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class DiscriminatorLoss(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, fake_pred, real_pred):\n",
    "        fake_target = torch.zeros_like(fake_pred)\n",
    "        real_target = torch.ones_like(real_pred)\n",
    "        fake_loss = self.loss_fn(fake_pred, fake_target)\n",
    "        real_loss = self.loss_fn(real_pred, real_target)\n",
    "        loss = (fake_loss + real_loss)/2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdizxzlYNZB8"
   },
   "outputs": [],
   "source": [
    "def train_pix2pix(train_dataloader, eval_dataloader, generator, discriminator, g_loss, d_loss, metric_fns, g_optimizer, d_optimizer, n_epochs):\n",
    "    # training loop\n",
    "    logdir = './tensorboard/net'\n",
    "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
    "\n",
    "    history = {}  # collects metrics at the end of each epoch\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # initialize metric list\n",
    "        metrics = {'g_loss': [], 'd_loss': [], 'val_loss': []}\n",
    "        for k, _ in metric_fns.items():\n",
    "            metrics[k] = []\n",
    "            metrics['val_'+k] = []\n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
    "\n",
    "        # Training\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        for (x,y) in pbar:\n",
    "            # Generator\n",
    "            fake_image = generator(x)\n",
    "            fake_pred = discriminator(fake_image, x)\n",
    "            generator_loss = g_loss(fake_image, y, fake_pred)\n",
    "\n",
    "            # Discriminator\n",
    "            fake_image = generator(x).detach()\n",
    "            fake_pred = discriminator(fake_image, x)\n",
    "            real_pred = discriminator(y, x)\n",
    "            discriminator_loss = d_loss(fake_pred, real_pred)\n",
    "\n",
    "\n",
    "            # Performing the parameter updates\n",
    "            g_optimizer.zero_grad()\n",
    "            generator_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            metrics['g_loss'].append(generator_loss.item())\n",
    "            metrics['d_loss'].append(discriminator_loss.item())\n",
    "\n",
    "            for k, fn in metric_fns.items():\n",
    "                metrics[k].append(fn(fake_image, y).item())\n",
    "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint_path = f'models/checkpoint_epoch_generator_{epoch + 1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': generator.state_dict(),\n",
    "                'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                'loss': g_loss,\n",
    "            }, checkpoint_path)\n",
    "            checkpoint_path = f'models/checkpoint_epoch_discriminator_{epoch + 1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                'loss': d_loss,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "\n",
    "        # validation\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "        with torch.no_grad():  # do not keep track of gradients\n",
    "            for (x, y) in eval_dataloader:\n",
    "                y_hat = generator(x)  # forward pass\n",
    "                fake_pred = discriminator(y_hat, x)\n",
    "                loss = g_loss(y_hat, y, fake_pred)\n",
    "\n",
    "                # log partial metrics\n",
    "                metrics['val_loss'].append(loss.item())\n",
    "                for k, fn in metric_fns.items():\n",
    "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
    "\n",
    "        # summarize metrics, log to tensorboard and display\n",
    "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
    "        for k, v in history[epoch].items():\n",
    "          writer.add_scalar(k, v, epoch)\n",
    "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
    "        show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
    "\n",
    "    print('Finished Training')\n",
    "    # plot loss curves\n",
    "    plt.plot([v['d_loss'] for k, v in history.items()], label='Discriminator Loss')\n",
    "    plt.plot([v['g_loss'] for k, v in history.items()], label='Generator Loss')\n",
    "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0ash-9bNZB8"
   },
   "outputs": [],
   "source": [
    "# Training with default Unet\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=0.1, random_state=42\n",
    ")\n",
    "train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
    "val_patches, val_labels = image_to_patches(val_images, val_masks)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "generator = UNet().to(device)\n",
    "discriminator = Discriminator(4).to(device)\n",
    "generator_loss = GeneratorLoss()\n",
    "discriminator_loss = DiscriminatorLoss()\n",
    "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
    "n_epochs = N_EPOCHS\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#train_pix2pix(train_dataloader, val_dataloader, generator, discriminator, generator_loss, discriminator_loss, metric_fns, g_optimizer, d_optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEEoKTGLNZB8"
   },
   "outputs": [],
   "source": [
    "# Training with UNETVGG\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "# Freeze all the layers\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze some of the later layers (uncomment if needed)\n",
    "for param in vgg16.features[24:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "vgg_features = list(vgg16.features.children())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=0.1, random_state=42\n",
    ")\n",
    "train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
    "val_patches, val_labels = image_to_patches(val_images, val_masks)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(RESIZE, RESIZE))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "generator = UNetVGG(vgg_features).to(device)\n",
    "discriminator = Discriminator(4).to(device)\n",
    "generator_loss = GeneratorLoss()\n",
    "discriminator_loss = DiscriminatorLoss()\n",
    "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
    "n_epochs = N_EPOCHS\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#train_pix2pix(train_dataloader, val_dataloader, generator, discriminator, generator_loss, discriminator_loss, metric_fns, g_optimizer, d_optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J3JZirbNZB8"
   },
   "outputs": [],
   "source": [
    "# Picking the best checkpoint by the validation score\n",
    "checkpoint = torch.load('model.pth', map_location=torch.device('cpu'))\n",
    "generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA92bcQENZB8"
   },
   "outputs": [],
   "source": [
    "test_path = os.path.join(ROOT_PATH, 'test', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(RESIZE, RESIZE)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [generator(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
    "test_pred = np.moveaxis(test_pred, 2, 3)\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
    "create_submission(test_pred, test_filenames, submission_filename='pix2pix_elu_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90zHBQu0NZB8"
   },
   "source": [
    "## Upgrade 2.3 -- CGAN with DCED Framework --> Road Segmentation of Remotely-Sensed Images Using Deep Convolutional Neural Networks with Landscape Metrics and Conditional Random Fields\n",
    "In this framework, the writers use 4 additional ideas.\n",
    "1. Using ELU activation function instead of RELU\n",
    "2. Using Gaussian Smoothing and Connected Component Labeling\n",
    "3. False Road Object Removal with LMs\n",
    "4. Road Object Sharpening with CRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65GJiSOtNZB9"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Defining the functions for the framework\n",
    "def gaussian_smoothing(kernel_size, sigma=1):\n",
    "  kernel_size = int(kernel_size) // 2\n",
    "  x, y = np.mgrid[-kernel_size:kernel_size+1, -kernel_size:kernel_size+1]\n",
    "  normal = 1 / (2.0 * np.pi * sigma**2)\n",
    "  g =  np.exp(-((x**2 + y**2) / (2.0*sigma**2))) * normal\n",
    "  return g\n",
    "\n",
    "def connected_component_labeling(prediction, gaussian_filter, threshold=128):\n",
    "    mask = np.uint8(prediction*255)\n",
    "    mask = cv2.filter2D(mask,-1,gaussian_filter)\n",
    "    _, binary_image = cv2.threshold(np.uint8(mask), threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=4)\n",
    "    # Stats is --> https://stackoverflow.com/questions/35854197/how-to-use-opencvs-connectedcomponentswithstats-in-python\n",
    "    \n",
    "    #print(f\"Number of labels: {num_labels}\")\n",
    "    #print(\"Stats: \")\n",
    "    #print(stats)\n",
    "    #print(\"Centroids: \")\n",
    "    #print(centroids)\n",
    "    return num_labels, labels, stats, centroids\n",
    "\n",
    "def calculate_shape_index(stats):\n",
    "  perimeter = 2 * (stats[2] + stats[3])\n",
    "  return perimeter / (4 * math.sqrt(stats[-1]))\n",
    "\n",
    "def remove_noise(image, num_labels, labels, stats, threshold=1.25, isprint=False):\n",
    "    output = copy.deepcopy(image)\n",
    "    # Map component labels to hue value\n",
    "    for label in range(1, num_labels):\n",
    "        mask = labels == label\n",
    "        index = calculate_shape_index(stats[label].tolist())\n",
    "        if isprint:\n",
    "            print(label, 'and', index)\n",
    "        if index < threshold:\n",
    "          output[mask] = 0 # removing the object\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Defining the class for the last step of DCED framework --> CRF\n",
    "class CRF():\n",
    "    def __init__(self, kernel_1_weight=10, kernel_2_weight=5, alpha=60, beta=10, gamma=1, efficient=False, spatial_downsampling=15, range_downsampling=15, iterations=3):\n",
    "        self.kernel_1_weight = kernel_1_weight\n",
    "        self.kernel_2_weight = kernel_2_weight\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.efficient = efficient\n",
    "        self.spatial_downsampling = spatial_downsampling\n",
    "        self.range_downsampling = range_downsampling\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def appearance_kernel(self, x_1, y_1, p_1, x_2, y_2, p_2):\n",
    "        \"\"\"Compute appearance kernel.\n",
    "    \n",
    "        Args:\n",
    "            x_1: X coordinate of first pixel.\n",
    "            y_1: Y coordinate of first pixel.\n",
    "            p_1: Color vector of first pixel.\n",
    "            x_2: X coordinate of second pixel.\n",
    "            y_2: Y coordinate of second pixel.\n",
    "            p_2: Color vector of second pixel.\n",
    "            theta_alpha: Standard deviation for the position.\n",
    "            theta_beta: Standard deviation for the color.\n",
    "    \n",
    "        Returns:\n",
    "            The output of the appearence kernel.\n",
    "        \"\"\"\n",
    "        result = np.exp(\n",
    "        -((x_1 - x_2) ** 2.0 + (y_1 - y_2) ** 2.0) / (2 * self.alpha ** 2.0)\n",
    "        - np.sum((p_1 - p_2) ** 2.0) / (2.0 * self.beta ** 2.0)\n",
    "        )\n",
    "        #print(f'Result of apperance kernel is {result}')\n",
    "        return result\n",
    "\n",
    "\n",
    "    def smoothness_kernel(self, x_1, y_1, p_1, x_2, y_2, p_2):\n",
    "        \"\"\"Compute smoothness kernel.\n",
    "    \n",
    "        Args:\n",
    "            x_1: X coordinate of first pixel.\n",
    "            y_1: Y coordinate of first pixel.\n",
    "            p_1: Color vector of first pixel.\n",
    "            x_2: X coordinate of second pixel.\n",
    "            y_2: Y coordinate of second pixel.\n",
    "            p_2: Color vector of second pixel.\n",
    "            theta_gamma: Standard deviation for the position.\n",
    "    \n",
    "        Returns:\n",
    "            The output of the smoothness kernel.\n",
    "        \"\"\"\n",
    "        del p_1, p_2\n",
    "        result = np.exp(\n",
    "            -((x_1 - x_2) ** 2.0 + (y_1 - y_2) ** 2.0) / (2.0 * self.gamma ** 2.0)\n",
    "        )\n",
    "        #print(f'Result of smoothness kernel {result}')\n",
    "        return result\n",
    "\n",
    "    def normalize(self, potentials):\n",
    "        \"\"\"Normalize potentials such that output is a valid pixelwise distribution.\n",
    "    \n",
    "        Args:\n",
    "            potentials: Array of potentials. Shape (H,W,N).\n",
    "    \n",
    "        Returns:\n",
    "            Probability array with same shape as potentials.\n",
    "            Probabilities sum up to 1 at every slice (i,j,:).\n",
    "        \"\"\"\n",
    "        # Sum the potentials along the last axis (the class axis)\n",
    "        sum_potentials = np.sum(potentials, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Avoid division by zero\n",
    "        sum_potentials[sum_potentials == 0] = 1\n",
    "    \n",
    "        # Normalize by dividing each potential by the sum of potentials at that pixel\n",
    "        normalized_potentials = potentials / sum_potentials\n",
    "    \n",
    "        return normalized_potentials\n",
    "\n",
    "    def message_passing(self, image, current_probabilities) :\n",
    "        \"\"\"Perform \"message passing\" as the first step of the inference loop.\n",
    "    \n",
    "        Args:\n",
    "            image:\n",
    "                Array of size ROWS x COLUMNS x CHANNELS, representing the image used to\n",
    "                compute the kernel.\n",
    "            current_probabilities:\n",
    "                Array of size ROWS x COLUMNS x CLASSES, representing the current\n",
    "                probabilities.\n",
    "            kernel_functions: The kernel functions defining the edge potential.\n",
    "    \n",
    "        Returns:\n",
    "            Array of size ROWS x COLUMNS x CLASSES x KERNELS, representing the intermediate\n",
    "            result of message passing for each kernel.\n",
    "        \"\"\"\n",
    "        # naive version\n",
    "        rows = image.shape[0]\n",
    "        cols = image.shape[1]\n",
    "        classes = current_probabilities.shape[2] # road or not\n",
    "        result = np.zeros(\n",
    "            (\n",
    "                current_probabilities.shape[0],\n",
    "                current_probabilities.shape[1],\n",
    "                classes, #1 class --> road or not\n",
    "                2, # 2 kernels\n",
    "            ),\n",
    "            dtype=float,\n",
    "        )\n",
    "        \n",
    "    \n",
    "        # TODO implement naive message passing (using loops)\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                probability_1 = 0\n",
    "                probability_2 = 0\n",
    "                color_vector_1 = image[i, j, :]\n",
    "                for k in range(rows):\n",
    "                    for l in range(cols):\n",
    "                        if (i == k) and (j == l):\n",
    "                            pass\n",
    "                        else:\n",
    "                            color_vector_2 = image[k, l, :]\n",
    "                            probability_1 = probability_1 + result[k, l, 0, 0] * self.appearance_kernel(i, j, color_vector_1, k, l, color_vector_2)\n",
    "                            probability_2 = probability_2 + result[k, l, 0, 1] * self.smoothness_kernel(i, j, color_vector_1, k, l, color_vector_2)\n",
    "                result[i, j, 0, 0] = probability_1\n",
    "                result[i, j, 0, 1] = probability_2\n",
    "                #print(f'----------- {i}, {j}, {probability_1}, {probability_2}')\n",
    "        return result\n",
    "\n",
    "    def compatibility_transform(self,q_tilde):\n",
    "        \"\"\"Perform compatability transform as part of the inference loop.\n",
    "    \n",
    "        Args:\n",
    "            q_tilde:\n",
    "                Array of size ROWS x COLUMNS x CLASSES x KERNELS, representing the\n",
    "                intermediate result of message passing for each kernel.\n",
    "            weights: Weights of each kernel.\n",
    "    \n",
    "        Returns:\n",
    "            Array of size ROWS x COLUMNS x CLASSES, representing the result after combining\n",
    "            the kernels and applying the label compatability function (here: Potts model).\n",
    "        \"\"\"\n",
    "    \n",
    "        # TODO: implement compatability transform (try with matrix operations only)\n",
    "        weights = [self.kernel_1_weight, self.kernel_2_weight]\n",
    "        q_tilde[..., 0] *= weights[0] \n",
    "        q_tilde[..., 1] *= weights[1]\n",
    "        result = np.sum(q_tilde, axis=-1)\n",
    "        return result\n",
    "\n",
    "    def get_unary_potential(self, image):\n",
    "        return -np.log(image)\n",
    "\n",
    "    def local_update(self, q_hat, unary_potential):\n",
    "        \"\"\"Perform local update as part of the interefence loop.\n",
    "    \n",
    "        Args:\n",
    "            q_hat:\n",
    "                Array of size ROWS x COLUMNS x CLASSES, representing the intermediate result\n",
    "                after combining the kernels and applying the label compatability function.\n",
    "            unary_potential:\n",
    "                Array of size ROWS x COLUMNS x CLASSES, representing the prior energy for\n",
    "                each pixel and class from a different source.\n",
    "        Returns:\n",
    "            Array of size ROWS x COLUMNS x CLASSES, representing the probabilities for each\n",
    "            pixel and class.\n",
    "        \"\"\"\n",
    "        result = np.exp(-unary_potential - q_hat)\n",
    "        #print(f'Local update result is {result}')\n",
    "        return np.exp(-unary_potential - q_hat)\n",
    "\n",
    "    def efficient_message_passing(self, image, current_probabilities):\n",
    "        \"\"\"Perform efficient \"message passing\" by downsampling and convolution in 5D.\n",
    "    \n",
    "        This assumes two kernels: an appearance kernel based on theta_alpha and theta_beta,\n",
    "        and a smoothness kernel based on theta_gamma.\n",
    "    \n",
    "        Args:\n",
    "            image:\n",
    "                Array of size ROWS x COLUMNS x CHANNELS, representing the image used to\n",
    "                compute the kernel.\n",
    "            current_probabilities:\n",
    "                Array of size ROWS x COLUMNS x CLASSES, representing the current\n",
    "                probabilities.\n",
    "            spatial_downsampling:\n",
    "                Factor to downsample the spatial dimensions for the 5D representation.\n",
    "            range_downsampling:\n",
    "                Factor to downsample the range dimensions for the 5D representation.\n",
    "            theta_alpha: Spatial standard deviation for the appearance kernel.\n",
    "            theta_beta: Color standard deviation for the appearance kernel.\n",
    "            theta_gamma: Spatial standard deviation for the smoothness kernel.\n",
    "    \n",
    "        Returns:\n",
    "            Array of size ROWS x COLUMNS x CLASSES x KERNELS, representing the intermediate\n",
    "            result of message passing for each kernel.\n",
    "        \"\"\"\n",
    "        #t_0 = time.time()\n",
    "    \n",
    "        rows = image.shape[0]\n",
    "        cols = image.shape[1]\n",
    "        classes = current_probabilities.shape[2]\n",
    "        color_range = 255\n",
    "    \n",
    "        ds_rows = int(np.ceil(rows / self.spatial_downsampling))\n",
    "        ds_cols = int(np.ceil(cols / self.spatial_downsampling))\n",
    "        ds_range = int(np.ceil(color_range / self.range_downsampling))\n",
    "    \n",
    "        #print(f\"Downsampled to: {ds_rows}x{ds_cols}x{ds_range}\")\n",
    "    \n",
    "        result = np.zeros(\n",
    "            (\n",
    "                current_probabilities.shape[0],\n",
    "                current_probabilities.shape[1],\n",
    "                current_probabilities.shape[2],\n",
    "                2,\n",
    "            ),\n",
    "            dtype=float,\n",
    "        )\n",
    "    \n",
    "        # Precompute indices\n",
    "        indices_list = []\n",
    "        for row in np.arange(rows):\n",
    "            for col in np.arange(cols):\n",
    "                indices_list.append(\n",
    "                    (row, col, image[row, col, 0], image[row, col, 1], image[row, col, 2])\n",
    "                )\n",
    "        indices_list = np.array(indices_list, dtype=float)\n",
    "        indices_list[:, 0:2] = indices_list[:, 0:2] / float(self.spatial_downsampling)\n",
    "        indices_list[:, 2:] = indices_list[:, 2:] / float(self.range_downsampling)\n",
    "        indices_list = np.round(indices_list).astype(int)\n",
    "\n",
    "        for class_id in np.arange(classes):\n",
    "            # Allocate 5D feature space\n",
    "            feature_space = np.zeros((ds_rows+1, ds_cols+1, ds_range+1, ds_range+1, ds_range+1))\n",
    "            # Downsample with box filter and go to 5D space at same time\n",
    "            for row in np.arange(rows):\n",
    "                for col in np.arange(cols):\n",
    "                    idx = indices_list[row * cols + col]\n",
    "                    feature_space[idx[0], idx[1], idx[2], idx[3], idx[4]] += current_probabilities[row, col, 0]\n",
    "    \n",
    "            for kernel_id in np.arange(2):\n",
    "                if kernel_id == 0:  # Appearance kernel\n",
    "                    # Apply appearance kernel as a Gaussian filter\n",
    "                    filtered_feature_space = gaussian_filter(feature_space, sigma=[self.alpha , self.alpha, self.beta, self.beta, self.beta])\n",
    "        \n",
    "                if kernel_id == 1:  # Smoothness kernel\n",
    "                    # Apply smoothness kernel as a Gaussian filter\n",
    "                    filtered_feature_space = gaussian_filter(feature_space, sigma=[self.gamma, self.gamma, 0, 0, 0])\n",
    "        \n",
    "                # Upsample with simple lookup (no interpolation for simplicity)\n",
    "                for row in np.arange(rows):\n",
    "                    for col in np.arange(cols):\n",
    "                        idx = indices_list[row * cols + col]\n",
    "                        result[row, col, 0, kernel_id] = filtered_feature_space[idx[0], idx[1], idx[2], idx[3], idx[4]]\n",
    "        \n",
    "        #t_1 = time.time()\n",
    "        #print(f\"Efficient message passing took {t_1-t_0}s\")\n",
    "    \n",
    "        return result\n",
    "    \n",
    "    def inference(self, image, initial_probabilities):\n",
    "        \"\"\"Perform inference in fully connected CRF with Gaussian edge potentials.\n",
    "    \n",
    "        Args:\n",
    "            image:\n",
    "                Array of size ROWS x COLUMNS x CHANNELS, representing the image used the\n",
    "                features.\n",
    "            initial_probabilities:\n",
    "                Initial pixelwise probabilities for each class. Used to initialize unary\n",
    "                potential.\n",
    "            params:\n",
    "                Parameter class for fully connected CRFs (see CrfParameters documentation).\n",
    "        Return:\n",
    "            Array of size ROWS x COLS x CLASSES\n",
    "        \"\"\"\n",
    "        # initialize\n",
    "        current_probabilities = initial_probabilities\n",
    "    \n",
    "        unary_potential = -np.log(current_probabilities)\n",
    "    \n",
    "        for _ in np.arange(self.iterations):\n",
    "            if self.efficient:\n",
    "                q_tilde = self.efficient_message_passing(image,current_probabilities)\n",
    "            else:\n",
    "                q_tilde = self.message_passing(image, current_probabilities)\n",
    "            q_hat = self.compatibility_transform(q_tilde)\n",
    "            unnormalized_probabilities = self.local_update(q_hat, unary_potential)\n",
    "            #print(unnormalized_probabilities)\n",
    "            current_probabilities = self.normalize(unnormalized_probabilities)\n",
    "            #print(current_probabilities)\n",
    "            print('Iteration completed')\n",
    "    \n",
    "        return current_probabilities\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmvCtlKQPbUS"
   },
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "model = generator\n",
    "test_path = os.path.join(ROOT_PATH, 'training', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "test_images2 = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(RESIZE, RESIZE)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "test_pred = test_pred.reshape(test_pred.shape[0], test_pred.shape[1], test_pred.shape[2], 1)\n",
    "\n",
    "# now compute labels\n",
    "#output = output.reshape(output.shape[0], output.shape[1], output.shape[2])\n",
    "# now compute labels\n",
    "#output = output.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
    "#output = np.moveaxis(output, 2, 3)\n",
    "#output = np.round(np.mean(output, (-1, -2)) > CUTOFF)\n",
    "#create_submission(output, test_filenames, submission_filename='pix2pix_dced_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "ju61itlwNZB9",
    "outputId": "0bfb48ce-e46e-4204-f0a1-0cc668dfd458"
   },
   "outputs": [],
   "source": [
    "# Applying gaussian blur + ccl + lm\n",
    "filter_size = 19 # Rule of thumb: size is 6 times standard deviation\n",
    "gaussian_filter = gaussian_smoothing(filter_size, sigma=3)\n",
    "\n",
    "output = []\n",
    "output_image = np.zeros((test_pred[4].shape[0], test_pred[20].shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(test_pred.shape[0]):\n",
    "    num_labels, labels, stats, centroids = connected_component_labeling(test_pred[i], gaussian_filter, threshold=128)\n",
    "    lm_output = None\n",
    "    if i == 4:\n",
    "        # Map component labels to hue value\n",
    "        for label in range(1, num_labels):\n",
    "            mask = labels == label\n",
    "            color = np.random.randint(0, 255, size=3)\n",
    "            output_image[mask] = color\n",
    "        lm_output = remove_noise(test_pred[i], num_labels, labels, stats, 1.3)\n",
    "    else:\n",
    "        lm_output = remove_noise(test_pred[i], num_labels, labels, stats, 1.3)\n",
    "    output.append(lm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images2 = test_images2[:, :, :, :3]\n",
    "test_images2 = test_images2*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_road_probabilities = 1 - output\n",
    "combined_probabilities = np.concatenate((output, non_road_probabilities), axis=-1)\n",
    "combined_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(efficient=True, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf2 = CRF(efficient=True, iterations=10, spatial_downsampling=1) #alpha=60, beta=10,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crfed = crf.inference(test_images2[0], combined_probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crfed_2 = crf2.inference(test_images2[0], combined_probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(crfed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsC0Es0jRYCq"
   },
   "outputs": [],
   "source": [
    "# Show the original and labeled images\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(crfed_2[:,:,0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Ilk Output')\n",
    "plt.imshow(test_pred[0])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('LM Output')\n",
    "plt.imshow(output[0])\n",
    "plt.title('CRF Output')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(crfed[:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the original and labeled images\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(test_pred[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Labeled Components')\n",
    "plt.imshow(output[0])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(crfed)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0356dacdf3244561981c3c3296064c75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b60d5aa9743440bb56cb0d1a2d55ef3",
      "placeholder": "",
      "style": "IPY_MODEL_66588237979c4f37b0388d8f7935959a",
      "value": "29/29[00:14&lt;00:00,2.12it/s,loss=0.349,acc=0.845,patch_acc=0.804]"
     }
    },
    "07545bbbe26846a8a71584d78aeabe8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0d20d0c0edf4ddd82d020c610b0e521",
      "placeholder": "",
      "style": "IPY_MODEL_c017935d04394cb6b204fe70237475f7",
      "value": "29/29[00:07&lt;00:00,4.29it/s,loss=0.389,acc=0.837,patch_acc=0.72]"
     }
    },
    "077ec395e1a94b8dbbb7401fd137e938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdcb03497ca74ca3b6dc83ddbc72a7cf",
       "IPY_MODEL_1a6c5e1e28a2436983d59cbd6be66b26",
       "IPY_MODEL_2f631587389a4346998862786282db3b"
      ],
      "layout": "IPY_MODEL_27e0f1464e6240e6bd5956ecd648a488"
     }
    },
    "1a453e1bd9b643ff8efe0bf42fb7fe90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a6c5e1e28a2436983d59cbd6be66b26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5523d1c62544d249c812a061624e0e2",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_648b769bea8e4776a0a60a46417707b7",
      "value": 29
     }
    },
    "1aae38846f6a4e98bb6eb9aa0db9ed1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7cbda34b30e4f3ebb2b9ec87b2cbf70",
      "placeholder": "",
      "style": "IPY_MODEL_a153cff3490644ffbc8411a98b918278",
      "value": "Epoch1/1:100%"
     }
    },
    "1b965d02378d47d4984150f470ee0f7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3b6afea44d64bd4a67d80aa2b2e26e1",
      "placeholder": "",
      "style": "IPY_MODEL_c88dea4fcea441d482ba814e61104ad9",
      "value": "Epoch3/5:100%"
     }
    },
    "1ed38be279154345bec6c11343104310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97d8800ae9664b70a2e8c2c2d498a70b",
      "placeholder": "",
      "style": "IPY_MODEL_a84edf51e17340d98a20654bc094588e",
      "value": "29/29[00:14&lt;00:00,2.19it/s,loss=0.445,acc=0.804,patch_acc=0.681]"
     }
    },
    "25f2669f1d934b85aec1c80c07cc2c49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "279ccf8516c24bca91440b6cefb7ec7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27cf0290ada948d3a6e1c9bc585f04ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27e0f1464e6240e6bd5956ecd648a488": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a9d95440d4f41b49ab7d795d01e51de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b0d5b96fd0b44c3b6461db9f00319f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c0f2e104af14fa0972e9941c06e1c44",
       "IPY_MODEL_693243a6461a44c98b097ad4e94d6e25",
       "IPY_MODEL_c8dfc3b594d8448587c99c32cada60bc"
      ],
      "layout": "IPY_MODEL_4b73254b718b4f11b69afcc1a0a6d8eb"
     }
    },
    "2b48843e344546b2823b412e2dcf7a32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0f2e104af14fa0972e9941c06e1c44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a430422ee594051aa17b5ff5d07d9bd",
      "placeholder": "",
      "style": "IPY_MODEL_27cf0290ada948d3a6e1c9bc585f04ba",
      "value": "Epoch2/5:100%"
     }
    },
    "2f631587389a4346998862786282db3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eea1fc7b01d94b8996f2706591866901",
      "placeholder": "",
      "style": "IPY_MODEL_7cfba533a1e44dbc87b85e6ea0f63991",
      "value": "29/29[00:14&lt;00:00,2.09it/s,loss=0.345,acc=0.852,patch_acc=0.805]"
     }
    },
    "3b4ae0766045450e924828ab1ab7f93c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "423c20f9bfdb474eb6adc063fefa8bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47739ef169734cbab8629ea09b647e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b48843e344546b2823b412e2dcf7a32",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b09cf406026b4503b6664668151f48d9",
      "value": 29
     }
    },
    "4824facb89cd4f999aaf242aae65804e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b73254b718b4f11b69afcc1a0a6d8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51cdd0c4cfc3482f8774b986e7c5159a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4b84f6c46f84522b80d04af0b057c72",
      "placeholder": "",
      "style": "IPY_MODEL_dc4a4b17c32848929405d8236e0f74bd",
      "value": "Epoch1/5:100%"
     }
    },
    "534234d0fd234606b48e73cffe531520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56e641bc23e3406db9bce2f685cbc594": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a430422ee594051aa17b5ff5d07d9bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b60d5aa9743440bb56cb0d1a2d55ef3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd18f19dd6f42dbadc012a74220b5c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b965d02378d47d4984150f470ee0f7d",
       "IPY_MODEL_d9fb7257a96a4397992d4d663f3a81a6",
       "IPY_MODEL_0356dacdf3244561981c3c3296064c75"
      ],
      "layout": "IPY_MODEL_f57012777045413b95cd6776d3ff7738"
     }
    },
    "61539a0719ab4f9c915507c2b6bff27a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51cdd0c4cfc3482f8774b986e7c5159a",
       "IPY_MODEL_ff06cff8ff5846e6ab3de7535049493b",
       "IPY_MODEL_1ed38be279154345bec6c11343104310"
      ],
      "layout": "IPY_MODEL_e58c26582b4a4f7e99fee9f8f81fad7a"
     }
    },
    "648b769bea8e4776a0a60a46417707b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66588237979c4f37b0388d8f7935959a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67440908c5034d2c9d51dd18437cf507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "693243a6461a44c98b097ad4e94d6e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c8cad8c0441442ea2e8d7600b93bdbf",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_279ccf8516c24bca91440b6cefb7ec7c",
      "value": 29
     }
    },
    "74d643dd49304b63aea0a753515393ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cfba533a1e44dbc87b85e6ea0f63991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97d8800ae9664b70a2e8c2c2d498a70b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "989ea4893a8244ae99ce87316e473132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_534234d0fd234606b48e73cffe531520",
      "placeholder": "",
      "style": "IPY_MODEL_b3f4b83f179d4aae87985421f95e4fac",
      "value": "Epoch5/5:100%"
     }
    },
    "9c8cad8c0441442ea2e8d7600b93bdbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ef0392622f41669fe810c57e3ff320": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a153cff3490644ffbc8411a98b918278": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4b84f6c46f84522b80d04af0b057c72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a84edf51e17340d98a20654bc094588e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a89d48bbf0964f6cbb79681c6514e2ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab84a877a6f54163920eefadebc62aac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_989ea4893a8244ae99ce87316e473132",
       "IPY_MODEL_b970c5ad54054df3930e6a87072ac8ca",
       "IPY_MODEL_d9193e4e55c34e11b14deb10fd7d4d20"
      ],
      "layout": "IPY_MODEL_1a453e1bd9b643ff8efe0bf42fb7fe90"
     }
    },
    "b09cf406026b4503b6664668151f48d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3f4b83f179d4aae87985421f95e4fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7cbda34b30e4f3ebb2b9ec87b2cbf70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b970c5ad54054df3930e6a87072ac8ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b4ae0766045450e924828ab1ab7f93c",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67440908c5034d2c9d51dd18437cf507",
      "value": 29
     }
    },
    "bdcb03497ca74ca3b6dc83ddbc72a7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25f2669f1d934b85aec1c80c07cc2c49",
      "placeholder": "",
      "style": "IPY_MODEL_423c20f9bfdb474eb6adc063fefa8bff",
      "value": "Epoch4/5:100%"
     }
    },
    "bf6c4c21f6b84304bfca7d513133d6d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c017935d04394cb6b204fe70237475f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5523d1c62544d249c812a061624e0e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c88dea4fcea441d482ba814e61104ad9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8dfc3b594d8448587c99c32cada60bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e198ffc23a994842831607f60301d752",
      "placeholder": "",
      "style": "IPY_MODEL_cc4e4b5cfbb74d69b5e6fcb25c8cbf41",
      "value": "29/29[00:14&lt;00:00,2.14it/s,loss=0.376,acc=0.839,patch_acc=0.789]"
     }
    },
    "cc4e4b5cfbb74d69b5e6fcb25c8cbf41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0d20d0c0edf4ddd82d020c610b0e521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d355a1a2a252455f90d5a2ef861f2d15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1aae38846f6a4e98bb6eb9aa0db9ed1e",
       "IPY_MODEL_47739ef169734cbab8629ea09b647e3d",
       "IPY_MODEL_07545bbbe26846a8a71584d78aeabe8d"
      ],
      "layout": "IPY_MODEL_bf6c4c21f6b84304bfca7d513133d6d9"
     }
    },
    "d9193e4e55c34e11b14deb10fd7d4d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74d643dd49304b63aea0a753515393ad",
      "placeholder": "",
      "style": "IPY_MODEL_2a9d95440d4f41b49ab7d795d01e51de",
      "value": "29/29[00:14&lt;00:00,2.08it/s,loss=0.329,acc=0.853,patch_acc=0.814]"
     }
    },
    "d9fb7257a96a4397992d4d663f3a81a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56e641bc23e3406db9bce2f685cbc594",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a89d48bbf0964f6cbb79681c6514e2ad",
      "value": 29
     }
    },
    "dc4a4b17c32848929405d8236e0f74bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e198ffc23a994842831607f60301d752": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3b6afea44d64bd4a67d80aa2b2e26e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e58c26582b4a4f7e99fee9f8f81fad7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eea1fc7b01d94b8996f2706591866901": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f57012777045413b95cd6776d3ff7738": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff06cff8ff5846e6ab3de7535049493b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ef0392622f41669fe810c57e3ff320",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4824facb89cd4f999aaf242aae65804e",
      "value": 29
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
