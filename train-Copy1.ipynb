{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "64YSZEYHvLWz",
    "tags": []
   },
   "source": [
    "# Road Segmentation Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mywPp16Gv3fI"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import parameters as params\n",
    "import utils\n",
    "import trainer\n",
    "from processing import augment\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from glob import glob\n",
    "from random import sample\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from train import train\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.datasets import ImageDataset\n",
    "#from utils.dataset import ImageDataset, load_all_data\n",
    "from utils.losses import DiceBCELoss\n",
    "from utils import utils \n",
    "from models.cgan import Discriminator, GeneratorLoss, DiscriminatorLoss\n",
    "from models.unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_seg import SETRModel, Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "images_org = utils.load_images(os.path.join(params.ROOT_PATH, 'training', 'images'), False)\n",
    "masks_org = utils.load_images(os.path.join(params.ROOT_PATH, 'training', 'groundtruth'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uptraining\n",
    "import sys\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "CHECKPOINT_PATH = Path(\"checkpoints\")\n",
    "DATA_PATH = Path(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SETRModel(patch_size=(32, 32), \n",
    "                    in_channels=3, \n",
    "                    out_channels=1, \n",
    "                    hidden_size=1024, \n",
    "                    sample_rate=5,\n",
    "                    num_hidden_layers=1, \n",
    "                    num_attention_heads=16, \n",
    "                    decode_features=[512, 256, 128, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.rand(1, 3, 384, 384).to(device)\n",
    "print(\"input: \" + str(t3.shape))\n",
    "\n",
    "print(\"output: \" + str(model2(t3).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2(t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 0)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "model = SETRModel(patch_size=(32, 32), \n",
    "                    in_channels=3, \n",
    "                    out_channels=1, \n",
    "                    hidden_size=1024, \n",
    "                    sample_rate=5,\n",
    "                    num_hidden_layers=1, \n",
    "                    num_attention_heads=16, \n",
    "                    decode_features=[512, 256, 128, 64])\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "metric_fns = {'acc': trainer.accuracy_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 100, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "test_path = os.path.join(params.ROOT_PATH, 'training', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = utils.load_all_from_path(test_path)\n",
    "test_images2 = utils.load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = utils.np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_submission(\"test\", \"images\",'setr_150_epoch.csv', model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 50, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(test_images2[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Ilk Output')\n",
    "plt.imshow(test_pred[0])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('LM Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ENsemble of Gans\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 0)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "generator = UNet({'channels':[3,64,128,256,512,1024], 'activation': 'RELU'}).to(device)\n",
    "discriminator = Discriminator(4).to(device)\n",
    "generator_loss = GeneratorLoss()\n",
    "discriminator_loss = DiscriminatorLoss()\n",
    "metric_fns = {'acc': trainer.accuracy_fn, 'patch_acc': trainer.patch_accuracy_fn}\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "trainer.train_pix2pix(train_dataloader, val_dataloader, generator, discriminator, generator_loss, discriminator_loss, metric_fns, g_optimizer, d_optimizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = []\n",
    "generators.append(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.nn.functional.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "        \n",
    "class GeneratorLoss2(nn.Module):\n",
    "    def __init__(self, alpha=100):\n",
    "        super().__init__()\n",
    "        self.alpha=alpha\n",
    "        self.bce=DiceLoss()\n",
    "        self.l1=nn.L1Loss()\n",
    "\n",
    "    def forward(self, fake, real, fake_pred):\n",
    "        fake_target = torch.ones_like(fake_pred)\n",
    "        loss = self.bce(fake_pred, fake_target) + self.alpha* self.l1(fake, real)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ENsemble of Gans\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "generator = UNet({'channels':[3,64,128,256,512,1024], 'activation': 'RELU'}).to(device)\n",
    "discriminator = Discriminator(4).to(device)\n",
    "generator_loss = GeneratorLoss2()\n",
    "discriminator_loss = DiscriminatorLoss()\n",
    "metric_fns = {'acc': trainer.accuracy_fn, 'patch_acc': trainer.patch_accuracy_fn}\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "trainer.train_pix2pix(train_dataloader, val_dataloader, generator, discriminator, generator_loss, discriminator_loss, metric_fns, g_optimizer, d_optimizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators.append(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "test_path = os.path.join(params.ROOT_PATH, 'test', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = utils.load_all_from_path(test_path)\n",
    "test_images2 = utils.load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = utils.np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [generators[1](t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydensecrf.densecrf as dcrf\n",
    "import numpy as np\n",
    "from pydensecrf.utils import create_pairwise_gaussian, create_pairwise_bilateral, unary_from_softmax\n",
    "\n",
    "def apply_dense_crf(model_pred, image, num_classes=2, iterations=10, sxy_gaussian=(3, 3), compat_gaussian=3):\n",
    "    \"\"\"\n",
    "    Apply DenseCRF to the probabilities of an image.\n",
    "\n",
    "    :param probabilities: The probability map of shape (num_classes, height, width)\n",
    "    :param image: The original image of shape (height, width, channels)\n",
    "    :param num_classes: Number of classes (default: 2 for binary classification)\n",
    "    :param iterations: Number of iterations for CRF inference\n",
    "    :param sxy_gaussian: Spatial kernel size for Gaussian kernel\n",
    "    :param compat_gaussian: Compatibility for Gaussian kernel\n",
    "    :return: Refined predictions\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    #probabilities = 1 / (1 + np.exp(-model_pred))\n",
    "    probabilities_2d = np.zeros((2, height, width), dtype=np.float32)\n",
    "    probabilities_2d[0, :, :] = 1 - model_pred\n",
    "    probabilities_2d[1, :, :] = model_pred\n",
    "\n",
    "    d = dcrf.DenseCRF2D(width, height, num_classes)\n",
    "\n",
    "    # The unary potential is negative log probability\n",
    "    unary = -np.log(probabilities_2d)\n",
    "    unary = unary.reshape((num_classes, -1))\n",
    "    d.setUnaryEnergy(unary)\n",
    "\n",
    "    # Add pairwise Gaussian\n",
    "    d.addPairwiseGaussian(sxy=sxy_gaussian, compat=compat_gaussian, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    image_uint8 = (image * 255).astype(np.uint8) if image.dtype == np.float32 else image.astype(np.uint8)\n",
    "\n",
    "    # Add pairwise Bilateral\n",
    "    d.addPairwiseBilateral(sxy=(80, 80), srgb=(13, 13, 13), rgbim=image_uint8, compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    # Perform inference\n",
    "    Q = d.inference(iterations)\n",
    "    result = np.argmax(Q, axis=0).reshape((height, width))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images2 = test_images2[:,:,:,:3]\n",
    "output = []\n",
    "for i, pred in enumerate(test_pred):\n",
    "    crf_result = apply_dense_crf(test_pred[i], test_images2[i], num_classes=2)\n",
    "    output.append(crf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(test_images2[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Ilk Output')\n",
    "plt.imshow(test_pred[0])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('LM Output')\n",
    "plt.imshow(output[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model4 = model4.to(device)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/neat-serenity-95/epoch_15.pt')\n",
    "model4.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "test_path = os.path.join(params.ROOT_PATH, 'test', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = utils.load_all_from_path(test_path)\n",
    "test_images2 = utils.load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = utils.np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [model4(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(test_images2[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Ilk Output')\n",
    "plt.imshow(test_pred[0])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('LM Output')\n",
    "plt.imshow(output[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "test_path = os.path.join(params.ROOT_PATH, 'test', 'images')\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = utils.load_all_from_path(test_path)\n",
    "test_images2 = utils.load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = utils.np_to_tensor(np.moveaxis(test_images, -1, 1), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model1 = model1.to(device)\n",
    "model2 = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model3 = smp.Unet(\n",
    "    encoder_name=\"vgg19\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model3 = model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/neat-serenity-95/epoch_15.pt')\n",
    "model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load('checkpoints/sage-shadow-94/epoch_15.pt')\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load('checkpoints/electric-water-93/epoch_15.pt')\n",
    "model3.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1 = [model1(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred1 = np.concatenate(test_pred1, 0)\n",
    "#print(test_pred.shape)\n",
    "test_pred1= np.moveaxis(test_pred1, 1, -1)\n",
    "\n",
    "test_pred2 = [model2(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred2 = np.concatenate(test_pred2, 0)\n",
    "#print(test_pred.shape)\n",
    "test_pred2= np.moveaxis(test_pred2, 1, -1)\n",
    "\n",
    "test_pred3 = [model3(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred3 = np.concatenate(test_pred3, 0)\n",
    "#print(test_pred.shape)\n",
    "test_pred3= np.moveaxis(test_pred3, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission on the test set\n",
    "tra_images = utils.load_images(os.path.join(params.ROOT_PATH, 'training', 'images'), False)\n",
    "gra_images = utils.load_images(os.path.join(params.ROOT_PATH, 'training', 'groundtruth'), True)\n",
    "\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "tra_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in tra_images], 0)\n",
    "gra_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in gra_images], 0)\n",
    "tra_images = tra_images[:, :, :, :3]\n",
    "tra_images = utils.np_to_tensor(np.moveaxis(tra_images, -1, 1), device)\n",
    "gra_images = utils.np_to_tensor(np.moveaxis(gra_images, -1, 1), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_pred1 = [model1(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "tra_pred1 = np.concatenate(tra_pred1, 0)\n",
    "#print(test_pred.shape)\n",
    "tra_pred1= np.moveaxis(tra_pred1, 1, -1)\n",
    "\n",
    "tra_pred2 = [model2(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "tra_pred2 = np.concatenate(tra_pred2, 0)\n",
    "#print(test_pred.shape)\n",
    "tra_pred2= np.moveaxis(tra_pred2, 1, -1)\n",
    "\n",
    "tra_pred3 = [model3(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "tra_pred3 = np.concatenate(tra_pred3, 0)\n",
    "#print(test_pred.shape)\n",
    "tra_pred3= np.moveaxis(tra_pred3, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_pred1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Combine base model predictions\n",
    "train_meta_features = np.stack((tra_pred1, tra_pred2, tra_pred3), axis=1)\n",
    "test_meta_features = np.stack((test_pred1, test_pred2, test_pred3), axis=1)\n",
    "\n",
    "# Train logistic regression as meta-model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(train_meta_features, gra_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_preds = meta_model.predict(test_meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"vgg19\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 40, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_backs = []\n",
    "model_backs.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 40, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_backs.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model4 = model4.to(device)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/lively-surf-85/epoch_40.pt')\n",
    "model4.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_backs.append(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for model_b in model_backs:\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "    \n",
    "    images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "    masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "    \n",
    "    val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "    val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "    \n",
    "    # reshape the image to simplify the handling of skip connections and maxpooling\n",
    "    train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "    val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "    \n",
    "        \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "        \n",
    "    loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "    metric_fns = {'acc': trainer.accuracy_fn,\n",
    "    'f1_score': trainer.f1_score_fn}\n",
    "    optimizer = torch.optim.Adam(model_b.parameters(), lr=1e-5)\n",
    "    #scheduler = ReduceLROnPlateau(optimizer)\n",
    "    train(model_b, optimizer, train_dataloader, val_dataloader, loss_fn, 15, None, 0, metric_fns)\n",
    "    models.append(model_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 40, None, 0, metric_fns)\n",
    "model_backs.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnext50_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 40, None, 0, metric_fns)\n",
    "model_backs.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_b in model_backs[-2:]:\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "    \n",
    "    images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "    masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "    \n",
    "    val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "    val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "    \n",
    "    # reshape the image to simplify the handling of skip connections and maxpooling\n",
    "    train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "    val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "    \n",
    "        \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "        \n",
    "    loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "    metric_fns = {'acc': trainer.accuracy_fn,\n",
    "    'f1_score': trainer.f1_score_fn}\n",
    "    optimizer = torch.optim.Adam(model_b.parameters(), lr=1e-5)\n",
    "    #scheduler = ReduceLROnPlateau(optimizer)\n",
    "    train(model_b, optimizer, train_dataloader, val_dataloader, loss_fn, 15, None, 0, metric_fns)\n",
    "    models.append(model_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"vgg19\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 40, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model4 = model4.to(device)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/lively-surf-85/epoch_40.pt')\n",
    "model4.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train2(model4, optimizer, images_org, masks_org, loss_fn, 15, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model1 = model1.to(device)\n",
    "model2 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model3 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model3 = model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/divine-thunder-64/epoch_15.pt')\n",
    "model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load('checkpoints/worldly-snow-83/epoch_15.pt')\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load('checkpoints/dutiful-donkey-84/epoch_15.pt')\n",
    "model3.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_submission(\"test\", \"images\",'worldly_snow.csv', model2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction\n",
    "test_path = os.path.join(params.ROOT_PATH, \"test\", \"images\")\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = utils.load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3]\n",
    "test_images = utils.np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "\n",
    "preds = utils.ensemble_predict(models, [1,1,1,1,1], test_images)\n",
    "\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in preds], 0)  # resize to original shape\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // params.PATCH_SIZE, params.PATCH_SIZE, size[0] // params.PATCH_SIZE, params.PATCH_SIZE))\n",
    "test_pred = np.moveaxis(test_pred, 2, 3)\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > params.CUTOFF)\n",
    "with open(\"ensemble_staged_training_5_diff.csv\", 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn, patch_array in zip(sorted(test_filenames), test_pred):\n",
    "            img_number = int(re.search(r\"satimage_(\\d+)\", fn).group(1))\n",
    "            for i in range(patch_array.shape[0]):\n",
    "                for j in range(patch_array.shape[1]):\n",
    "                    f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, j*params.PATCH_SIZE, i*params.PATCH_SIZE, int(patch_array[i, j])))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "model2 = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model2 = model2.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model2, optimizer, train_dataloader, val_dataloader, loss_fn, 20, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-5)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model2, optimizer, train_dataloader, val_dataloader, loss_fn, 15, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 3)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-5)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model2, optimizer, train_dataloader, val_dataloader, loss_fn, 15, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet101\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 20, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 20, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 1)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 40, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,\n",
    "'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 15, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 20, None, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "\n",
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.VerticalFlip(),\n",
    "        tta.Rotate90([0,90])\n",
    "    ]\n",
    ")\n",
    "\n",
    "tta_models = []\n",
    "for i in range(5):\n",
    "    tta_models.append(tta.SegmentationTTAWrapper(models[i], transforms))\n",
    "\n",
    "preds = utils.ensemble_predict(tta_models, [1,1,1,1,1], test_images)\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in preds], 0)  # resize to original shape\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // params.PATCH_SIZE, params.PATCH_SIZE, size[0] // params.PATCH_SIZE, params.PATCH_SIZE))\n",
    "test_pred = np.moveaxis(test_pred, 2, 3)\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > params.CUTOFF)\n",
    "with open(\"ensemble_resunet_50_5_aug_tta.csv\", 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn, patch_array in zip(sorted(test_filenames), test_pred):\n",
    "            img_number = int(re.search(r\"satimage_(\\d+)\", fn).group(1))\n",
    "            for i in range(patch_array.shape[0]):\n",
    "                for j in range(patch_array.shape[1]):\n",
    "                    f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, j*params.PATCH_SIZE, i*params.PATCH_SIZE, int(patch_array[i, j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_submission(\"test\", \"images\",'resnet_trained_further.csv', model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "\n",
    "model = models[0].to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 20, scheduler, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_submission(\"test\", \"images\",'resnet_trained_even_further.csv', model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        images_org, masks_org, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "images_aug, masks_aug = augment.augment_data(train_images, train_masks, 2)\n",
    "\n",
    "images_aug = np.stack([img/255.0 for img in images_aug]).astype(np.float32)\n",
    "masks_aug = np.stack([mask/255.0 for mask in masks_aug]).astype(np.float32)\n",
    "\n",
    "val_images = np.stack([img/255.0 for img in val_images]).astype(np.float32)\n",
    "val_masks = np.stack([mask/255.0 for mask in val_masks]).astype(np.float32)\n",
    "\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset(images_aug, masks_aug, device, use_patches=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset(val_images, val_masks, device, use_patches=False, resize_to=(384, 384))\n",
    "\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "\n",
    "model = models[1].to(device)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "metric_fns = {'acc': trainer.accuracy_fn,'f1_score': trainer.f1_score_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer)\n",
    "train(model, optimizer, train_dataloader, val_dataloader, loss_fn, 15, scheduler, 0, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0356dacdf3244561981c3c3296064c75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b60d5aa9743440bb56cb0d1a2d55ef3",
      "placeholder": "​",
      "style": "IPY_MODEL_66588237979c4f37b0388d8f7935959a",
      "value": " 29/29 [00:14&lt;00:00,  2.12it/s, loss=0.349, acc=0.845, patch_acc=0.804]"
     }
    },
    "07545bbbe26846a8a71584d78aeabe8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0d20d0c0edf4ddd82d020c610b0e521",
      "placeholder": "​",
      "style": "IPY_MODEL_c017935d04394cb6b204fe70237475f7",
      "value": " 29/29 [00:07&lt;00:00,  4.29it/s, loss=0.389, acc=0.837, patch_acc=0.72]"
     }
    },
    "077ec395e1a94b8dbbb7401fd137e938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdcb03497ca74ca3b6dc83ddbc72a7cf",
       "IPY_MODEL_1a6c5e1e28a2436983d59cbd6be66b26",
       "IPY_MODEL_2f631587389a4346998862786282db3b"
      ],
      "layout": "IPY_MODEL_27e0f1464e6240e6bd5956ecd648a488"
     }
    },
    "1a453e1bd9b643ff8efe0bf42fb7fe90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a6c5e1e28a2436983d59cbd6be66b26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5523d1c62544d249c812a061624e0e2",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_648b769bea8e4776a0a60a46417707b7",
      "value": 29
     }
    },
    "1aae38846f6a4e98bb6eb9aa0db9ed1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7cbda34b30e4f3ebb2b9ec87b2cbf70",
      "placeholder": "​",
      "style": "IPY_MODEL_a153cff3490644ffbc8411a98b918278",
      "value": "Epoch 1/1: 100%"
     }
    },
    "1b965d02378d47d4984150f470ee0f7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3b6afea44d64bd4a67d80aa2b2e26e1",
      "placeholder": "​",
      "style": "IPY_MODEL_c88dea4fcea441d482ba814e61104ad9",
      "value": "Epoch 3/5: 100%"
     }
    },
    "1ed38be279154345bec6c11343104310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97d8800ae9664b70a2e8c2c2d498a70b",
      "placeholder": "​",
      "style": "IPY_MODEL_a84edf51e17340d98a20654bc094588e",
      "value": " 29/29 [00:14&lt;00:00,  2.19it/s, loss=0.445, acc=0.804, patch_acc=0.681]"
     }
    },
    "25f2669f1d934b85aec1c80c07cc2c49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "279ccf8516c24bca91440b6cefb7ec7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27cf0290ada948d3a6e1c9bc585f04ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27e0f1464e6240e6bd5956ecd648a488": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a9d95440d4f41b49ab7d795d01e51de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b0d5b96fd0b44c3b6461db9f00319f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c0f2e104af14fa0972e9941c06e1c44",
       "IPY_MODEL_693243a6461a44c98b097ad4e94d6e25",
       "IPY_MODEL_c8dfc3b594d8448587c99c32cada60bc"
      ],
      "layout": "IPY_MODEL_4b73254b718b4f11b69afcc1a0a6d8eb"
     }
    },
    "2b48843e344546b2823b412e2dcf7a32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0f2e104af14fa0972e9941c06e1c44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a430422ee594051aa17b5ff5d07d9bd",
      "placeholder": "​",
      "style": "IPY_MODEL_27cf0290ada948d3a6e1c9bc585f04ba",
      "value": "Epoch 2/5: 100%"
     }
    },
    "2f631587389a4346998862786282db3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eea1fc7b01d94b8996f2706591866901",
      "placeholder": "​",
      "style": "IPY_MODEL_7cfba533a1e44dbc87b85e6ea0f63991",
      "value": " 29/29 [00:14&lt;00:00,  2.09it/s, loss=0.345, acc=0.852, patch_acc=0.805]"
     }
    },
    "3b4ae0766045450e924828ab1ab7f93c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "423c20f9bfdb474eb6adc063fefa8bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47739ef169734cbab8629ea09b647e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b48843e344546b2823b412e2dcf7a32",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b09cf406026b4503b6664668151f48d9",
      "value": 29
     }
    },
    "4824facb89cd4f999aaf242aae65804e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b73254b718b4f11b69afcc1a0a6d8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51cdd0c4cfc3482f8774b986e7c5159a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4b84f6c46f84522b80d04af0b057c72",
      "placeholder": "​",
      "style": "IPY_MODEL_dc4a4b17c32848929405d8236e0f74bd",
      "value": "Epoch 1/5: 100%"
     }
    },
    "534234d0fd234606b48e73cffe531520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56e641bc23e3406db9bce2f685cbc594": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a430422ee594051aa17b5ff5d07d9bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b60d5aa9743440bb56cb0d1a2d55ef3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd18f19dd6f42dbadc012a74220b5c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b965d02378d47d4984150f470ee0f7d",
       "IPY_MODEL_d9fb7257a96a4397992d4d663f3a81a6",
       "IPY_MODEL_0356dacdf3244561981c3c3296064c75"
      ],
      "layout": "IPY_MODEL_f57012777045413b95cd6776d3ff7738"
     }
    },
    "61539a0719ab4f9c915507c2b6bff27a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51cdd0c4cfc3482f8774b986e7c5159a",
       "IPY_MODEL_ff06cff8ff5846e6ab3de7535049493b",
       "IPY_MODEL_1ed38be279154345bec6c11343104310"
      ],
      "layout": "IPY_MODEL_e58c26582b4a4f7e99fee9f8f81fad7a"
     }
    },
    "648b769bea8e4776a0a60a46417707b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66588237979c4f37b0388d8f7935959a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67440908c5034d2c9d51dd18437cf507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "693243a6461a44c98b097ad4e94d6e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c8cad8c0441442ea2e8d7600b93bdbf",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_279ccf8516c24bca91440b6cefb7ec7c",
      "value": 29
     }
    },
    "74d643dd49304b63aea0a753515393ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cfba533a1e44dbc87b85e6ea0f63991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97d8800ae9664b70a2e8c2c2d498a70b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "989ea4893a8244ae99ce87316e473132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_534234d0fd234606b48e73cffe531520",
      "placeholder": "​",
      "style": "IPY_MODEL_b3f4b83f179d4aae87985421f95e4fac",
      "value": "Epoch 5/5: 100%"
     }
    },
    "9c8cad8c0441442ea2e8d7600b93bdbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ef0392622f41669fe810c57e3ff320": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a153cff3490644ffbc8411a98b918278": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4b84f6c46f84522b80d04af0b057c72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a84edf51e17340d98a20654bc094588e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a89d48bbf0964f6cbb79681c6514e2ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab84a877a6f54163920eefadebc62aac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_989ea4893a8244ae99ce87316e473132",
       "IPY_MODEL_b970c5ad54054df3930e6a87072ac8ca",
       "IPY_MODEL_d9193e4e55c34e11b14deb10fd7d4d20"
      ],
      "layout": "IPY_MODEL_1a453e1bd9b643ff8efe0bf42fb7fe90"
     }
    },
    "b09cf406026b4503b6664668151f48d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3f4b83f179d4aae87985421f95e4fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7cbda34b30e4f3ebb2b9ec87b2cbf70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b970c5ad54054df3930e6a87072ac8ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b4ae0766045450e924828ab1ab7f93c",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67440908c5034d2c9d51dd18437cf507",
      "value": 29
     }
    },
    "bdcb03497ca74ca3b6dc83ddbc72a7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25f2669f1d934b85aec1c80c07cc2c49",
      "placeholder": "​",
      "style": "IPY_MODEL_423c20f9bfdb474eb6adc063fefa8bff",
      "value": "Epoch 4/5: 100%"
     }
    },
    "bf6c4c21f6b84304bfca7d513133d6d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c017935d04394cb6b204fe70237475f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5523d1c62544d249c812a061624e0e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c88dea4fcea441d482ba814e61104ad9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8dfc3b594d8448587c99c32cada60bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e198ffc23a994842831607f60301d752",
      "placeholder": "​",
      "style": "IPY_MODEL_cc4e4b5cfbb74d69b5e6fcb25c8cbf41",
      "value": " 29/29 [00:14&lt;00:00,  2.14it/s, loss=0.376, acc=0.839, patch_acc=0.789]"
     }
    },
    "cc4e4b5cfbb74d69b5e6fcb25c8cbf41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0d20d0c0edf4ddd82d020c610b0e521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d355a1a2a252455f90d5a2ef861f2d15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1aae38846f6a4e98bb6eb9aa0db9ed1e",
       "IPY_MODEL_47739ef169734cbab8629ea09b647e3d",
       "IPY_MODEL_07545bbbe26846a8a71584d78aeabe8d"
      ],
      "layout": "IPY_MODEL_bf6c4c21f6b84304bfca7d513133d6d9"
     }
    },
    "d9193e4e55c34e11b14deb10fd7d4d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74d643dd49304b63aea0a753515393ad",
      "placeholder": "​",
      "style": "IPY_MODEL_2a9d95440d4f41b49ab7d795d01e51de",
      "value": " 29/29 [00:14&lt;00:00,  2.08it/s, loss=0.329, acc=0.853, patch_acc=0.814]"
     }
    },
    "d9fb7257a96a4397992d4d663f3a81a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56e641bc23e3406db9bce2f685cbc594",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a89d48bbf0964f6cbb79681c6514e2ad",
      "value": 29
     }
    },
    "dc4a4b17c32848929405d8236e0f74bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e198ffc23a994842831607f60301d752": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3b6afea44d64bd4a67d80aa2b2e26e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e58c26582b4a4f7e99fee9f8f81fad7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eea1fc7b01d94b8996f2706591866901": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f57012777045413b95cd6776d3ff7738": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff06cff8ff5846e6ab3de7535049493b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ef0392622f41669fe810c57e3ff320",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4824facb89cd4f999aaf242aae65804e",
      "value": 29
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
